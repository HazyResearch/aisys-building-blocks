# (WIP) Building Blocks for AI Systems - UNDER CONSTRUCTION

**This repo is under construction!**

This is a (biased) view of great work studying the building blocks of foundation models.
This Github was originally put together as a place to aggregate materials for a [NeurIPS keynote](https://neurips.cc/virtual/2023/invited-talk/73990) - but we're also hoping to highlight great work across ML Systems.
If you think we're missing something, please open an issue or PR!

<!-- **I'm currently just listing out links to everything referenced in the slides, but maybe we organize by topic?** -->

## Blog Posts
* [FlashAttention Blog](https://crfm.stanford.edu/2023/01/13/flashattention.html)
* [Simplifying S4](https://hazyresearch.stanford.edu/blog/2022-06-11-simplifying-s4â€‹)
* [FlashFFTConv](https://hazyresearch.stanford.edu/blog/2023-11-13-flashfftconv)
* MQAR blog
* Based blog
* M2-retrieval blog
* Sub-quadratic in model dimension blog
* Convolutions blog

## Courses and Slides
* [Sasha's talk on do we need attention?](https://github.com/srush/do-we-need-attention/blob/main/DoWeNeedAttention.pdf)
* [CS 324 lecture on autoregressive LLMs](https://stanford-cs324.github.io/winter2022/lectures/introduction/)
* Azalia course
* MLSys Seminar
* CS 324, both iterations

## Papers
* [Holoclean: Holistic Data Repairs with Probabilistic Inference](https://arxiv.org/abs/1702.00820)
* [Can Foundation Models Wrangle Your Data?](https://arxiv.org/abs/2205.09911)
* [FlashAttention](https://arxiv.org/abs/2205.14135) and [FlashAttention-2](https://arxiv.org/abs/2307.08691)
* [Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/abs/2111.00396)
* [FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores](https://arxiv.org/abs/2311.05908)
* S5 paper (find link)
* Mega paper
* RWKV paper
* GSS, DSS, RetNet, LRU, MultiRes, CKConv...
* [Hungry Hungry Hippos: Towards Language Modeling with State Space Models](https://arxiv.org/abs/2212.14052)
* [Hyena Hierarchy: Towards Larger Convolutional Language Models](https://arxiv.org/abs/2302.10866)
* Associative recall papers: Ba et al, Zhang and Zhou 17, Olsson et al 21

## Pointers to inference work
* MQA, GQA, Flash Decoding, VLLM, Spec Decoding, Matformer, TGI, TensorRT

## Pointers to high throughput work
* FlexGen, Evaporate, blog post

## Pointers to data types
* HyenaDNA, find papers that have used it, blog posts, code
